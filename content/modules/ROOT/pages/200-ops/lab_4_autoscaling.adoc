== Introduction

ROSA Cluster Autoscaler is a feature that helps automatically adjust the size of an ROSA cluster based on the current workload and resource demands. Cluster Autoscaler offers automatic and intelligent scaling of ROSA clusters, leading to efficient resource utilization, improved application performance, high availability, and simplified cluster management. By dynamically adjusting the cluster size based on workload demands, it helps organizations optimize their infrastructure costs while ensuring optimal application performance and scalability. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.

image::diagram-cluster-autoscaler.png[Diagram illustrating the cluster autoscaler process]

To learn more about cluster autoscaling, visit the https://docs.openshift.com/rosa/rosa_cluster_admin/rosa_nodes/rosa-nodes-about-autoscaling-nodes.html[Red Hat documentation for cluster autoscaling,window=_blank].

:numbered:
== Enable Autoscaling on the Default MachinePool

You can enable autoscaling on your cluster using either the `rosa` CLI or the Red Hat OpenShift Cluster Manager. Because you do not have credentials for the Red Hat OpenShift Cluster Manager you will be using the CLI in this lab. There are instructions at the end of the lab showing how to do it in the console.

You will need to set up autoscaling for each MachinePool in the cluster separately.

. To identify the machine pool IDs in a cluster, enter the following command:
+
[source,sh,role=execute]
----
rosa list machinepools --cluster rosa-$GUID
----
+
ifeval::["{rosa_deploy_hcp}" == "false"]
.Sample Output
[source,text,options=nowrap]
----
ID      AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONES    SUBNETS    SPOT INSTANCES  DISK SIZE  
worker  No           2         m5.xlarge                          us-east-2a                       No              300 GiB
----
+
The *ID* of the MachinePool that you want to add autoscaling to is `worker`.
endif::[]
ifeval::["{rosa_deploy_hcp}" == "true"]
.Sample Output
[source,text,options=nowrap]
----
ID       AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONE  SUBNET                    VERSION  AUTOREPAIR  
workers  No           2/2       m5.xlarge                          us-east-2a         subnet-02ee20ca64bb93535  4.14.1   Yes
----
+
The *ID* of the MachinePool that you want to add autoscaling to is `workers`.
endif::[]

. To enable autoscaling on a machine pool, enter the following command:
+
ifeval::["{rosa_deploy_hcp}" == "false"]
[source,sh,role=execute]
----
rosa edit machinepool --cluster rosa-$GUID worker --enable-autoscaling --min-replicas=2 --max-replicas=4
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Updated machine pool 'worker' on cluster 'rosa-6n4s8'
----
endif::[]
ifeval::["{rosa_deploy_hcp}" == "true"]
[source,sh,role=execute]
----
rosa edit machinepool --cluster rosa-$GUID workers --enable-autoscaling --min-replicas=2 --max-replicas=4
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Updated machine pool 'workers' on cluster 'rosa-6n4s8'
----
endif::[]

ifeval::["{rosa_deploy_hcp}" == "false"]
. Next, let's check to see that our managed machine autoscalers have been created.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machineautoscaler
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                 REF KIND     REF NAME                             MIN   MAX   AGE
rosa-82prr-dw6cz-worker-us-east-2a   MachineSet   rosa-82prr-dw6cz-worker-us-east-2a   2     4     58s
----

. And finally, let's check to see that our cluster autoscaler has been created.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc get clusterautoscaler
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME      AGE
default   4m55s
----
endif::[]

== Test the Cluster Autoscaler

Now let's test the cluster autoscaler and see it in action.
To do so, we'll deploy a job with a load that this cluster cannot handle.
This should force the cluster to scale to handle the load.

. First, let's create a namespace (also known as a project in OpenShift).
To do so, run the following command:
+
[source,sh,role=execute]
----
oc new-project autoscale-ex
----
+
.Sample Output
[source,text,options=nowrap]
----
Now using project "autoscale-ex" on server "https://api.rosa-6n4s8.1c1c.p1.openshiftapps.com:6443".

You can add applications to this project with the 'new-app' command. For example, try:

    oc new-app rails-postgresql-example

to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application:

    kubectl create deployment hello-node --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 -- /agnhost serve-hostname
----

. Next, let's deploy our job that will exhaust the cluster's resources and cause it to scale more worker nodes.
To do so, run the following command:
+
[source,sh,role=execute]
----
cat << EOF | oc apply -f -
---
apiVersion: batch/v1
kind: Job
metadata:
  name: maxscale
  namespace: autoscale-ex
spec:
  template:
    spec:
      containers:
      - name: work
        image: busybox
        command: ["sleep",  "300"]
        resources:
          requests:
            memory: 500Mi
            cpu: 500m
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
      restartPolicy: Never
  backoffLimit: 4
  completions: 50
  parallelism: 50
EOF
----
+
.Sample Output
[source,text,options=nowrap]
----
job.batch/maxscale created
----

. After a few seconds, run the following to see what pods have been created.
+
[source,sh,role=execute]
----
oc -n autoscale-ex get pods
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME             READY   STATUS    RESTARTS   AGE
maxscale-2c6zt   1/1     Running   0          29s
maxscale-2ps5g   0/1     Pending   0          29s
maxscale-42l2d   0/1     Pending   0          29s
maxscale-4n8rt   0/1     Pending   0          29s
maxscale-5888n   1/1     Running   0          29s

[...Output Omitted...]
----
+
Notice that we see a lot of pods in a pending state.
This should trigger the cluster autoscaler to create more machines using the MachineAutoscaler we created.

. It will take a few minutes (like 5 minutes) for the new nodes to be available.

ifeval::["{rosa_deploy_hcp}" == "false"]
. Check the number of nodes in your cluster (repeat this command until you see 9 nodes - 3 control plane nodes, 2 infra nodes and the maximum 4 that you configured for autoscaling the Machinepool):
+
[source,sh,role=execute]
----
oc get nodes
----
+
.Sample Output
[source,texinfo]
----
NAME                                       STATUS   ROLES    AGE     VERSION
ip-10-0-0-183.us-east-2.compute.internal   Ready    worker   59s     v1.25.12+ba5cc25
ip-10-0-0-245.us-east-2.compute.internal   Ready    worker   3h49m   v1.25.12+ba5cc25
ip-10-0-0-249.us-east-2.compute.internal   Ready    worker   69s     v1.25.12+ba5cc25
ip-10-0-0-53.us-east-2.compute.internal    Ready    worker   3h49m   v1.25.12+ba5cc25
----

. Let's check to see if our MachineSet automatically scaled (it may take a few minutes).
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machinesets
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                 DESIRED   CURRENT   READY   AVAILABLE   AGE
rosa-82prr-dw6cz-infra-us-east-2a    2         2         2       2           4h29m
rosa-82prr-dw6cz-worker-us-east-2a   4         4         2       2           4h49m
----
+
This shows that the cluster autoscaler is working on scaling multiple MachineSets up to 4.

. Now let's watch the cluster autoscaler create and delete machines as necessary (it may take several minutes for machines to appear in the Running state).
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machines \
  -l machine.openshift.io/cluster-api-machine-role=worker
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                       PHASE         TYPE        REGION      ZONE         AGE
rosa-82prr-dw6cz-worker-us-east-2a-g8f5m   Running       m5.xlarge   us-east-2   us-east-2a   4h46m
rosa-82prr-dw6cz-worker-us-east-2a-q6l9r   Running       m5.xlarge   us-east-2   us-east-2a   5m12s
rosa-82prr-dw6cz-worker-us-east-2a-qh5q4   Provisioned   m5.xlarge   us-east-2   us-east-2a   5m12s
rosa-82prr-dw6cz-worker-us-east-2a-stwdg   Running       m5.xlarge   us-east-2   us-east-2a   4h46m
----
endif::[]
ifeval::["{rosa_deploy_hcp}" == "true"]
. Check the number of nodes in your cluster. Repeat this command until you see 4 nodes - the maximum that you configured for autoscaling the Machinepool. It will take a few minutes (like 5) for the new nodes to be available.
+
[source,sh,role=execute]
----
oc get nodes
----
+
.Sample Output
[source,texinfo]
----
NAME                                       STATUS   ROLES    AGE     VERSION
ip-10-0-0-102.us-east-2.compute.internal   Ready    worker   2m9s    v1.27.6+f67aeb3
ip-10-0-0-140.us-east-2.compute.internal   Ready    worker   2m11s   v1.27.6+f67aeb3
ip-10-0-0-29.us-east-2.compute.internal    Ready    worker   127m    v1.27.6+f67aeb3
ip-10-0-0-62.us-east-2.compute.internal    Ready    worker   128m    v1.27.6+f67aeb3
----
endif::[]

. Once the nodes are available re-run the command to display the pods for the job. You should see that more pods are now running. If you still see some pods in Pending state that is normal because even 4 worker nodes may not be enough to handle the node - but you limited the autoscaler to 4 worker nodes.
+
[source,sh,role=execute]
----
oc -n autoscale-ex get pods
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME             READY   STATUS              RESTARTS   AGE
maxscale-2c6zt   0/1     Completed           0          5m18s
maxscale-2ps5g   0/1     ContainerCreating   0          5m18s
maxscale-42l2d   0/1     ContainerCreating   0          5m18s
maxscale-4n8rt   0/1     Pending             0          5m18s
maxscale-5888n   0/1     Completed           0          5m18s
maxscale-5944p   0/1     Completed           0          5m18s
maxscale-5nwfz   0/1     Pending             0          5m18s
maxscale-5p2n8   0/1     ContainerCreating   0          5m18s

[...Output omitted...]
----

*Congratulations!*

You've successfully demonstrated cluster autoscaling.

== Summary

Here you learned:

* Enable autoscaling on the default Machine Pool for your cluster
* Deploy an application on the cluster and watch the cluster autoscaler scale your cluster to support the increased workload

== Enable Autoscaling via Red Hat OpenShift Cluster Manager Console

[WARNING]
====
This section is for your information only. You do *not* have access to the OpenShift Cluster Manager. Feel free to read through these instructions to understand how to do it via the console - or skip to the next swection.
====

. Log back into the https://console.redhat.com/openshift[OpenShift Cluster Manager].
. In the Cluster section, locate your cluster and click on it.
+
image::ocm-cluster-list.png[OCM - Cluster List]

. Next, click on the _Machine pools_ tab.
+
image::ocm-cluster-detail-overview.png[OCM - Cluster Detail Overview]

. Next, click on the â‹® icon beside the _Default_ machine pool, and select _Scale_.
+
image::ocm-machine-pool-three-dots.png[OCM - Machine Pool Menu]

. Finally, check the _Enable autoscaling_ checkbox, and set the minimum to `1` and maximum to `2`, then click _Apply_.
+
image::ocm-machine-pool-scale-menu.png[OCM - Machine Pool Scale Menu]
