== Introduction

When deploying your ROSA cluster, you can configure many aspects of your worker nodes, but what happens when you need to change your worker nodes after they've already been created? These activities include scaling the number of nodes, changing the instance type, adding labels or taints, just to name a few.

Many of these changes are done using Machine Pools. Machine Pools ensure that a specified number of Machine replicas are running at any given time. Think of a Machine Pool as a "template" for the kinds of Machines that make up the worker nodes of your cluster. If you'd like to learn more, see the https://docs.openshift.com/rosa/rosa_cluster_admin/rosa_nodes/rosa-managing-worker-nodes.html[Red Hat documentation on worker node management,window=_blank].

Here are some of the advantages of using ROSA Machine Pools to manage the size of your cluster

* Scalability - ROSA Machine Pool enables horizontal scaling of your cluster. It can easily add or remove workers to handle the changes in workload. This flexibility ensures that your cluster can dynamically scale to meet the needs of your applications
* High Availability - ROSA Machine Pool supports the creation of 3 replicas of workers across different availability zones. This redundancy helps ensure high availability of applications by distributing workloads.
* Infrastructure Diversity - ROSA Machine Pool allows you to provision worker nodes of different instance type. This enables you you leverage the best kind of instance family for different workloads.
* Integration with Cluster Autoscaler - ROSA Machine Pool seamlessly integrates with the Cluster Autoscaler feature, which automatically adjusts the number of worker nodes based on the current demand. This integration ensures efficient resource utilization by scaling the cluster up or down as needed, optimizing costs and performance.

image::scale_machinepool.png[scale_machinepool]

:numbered:
== Scaling worker nodes

=== Via the CLI

. First, let's see what MachinePools already exist in our cluster. To do so, run the following command:
+
[source,sh,role=execute]
----
rosa list machinepools -c rosa-$GUID
----
+
ifeval::["{rosa_deploy_hcp}" == "false"]
.Sample Output
[source,text,options=nowrap]
----
ID      AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONES    SUBNETS    SPOT INSTANCES  DISK SIZE
worker  No           2         m5.xlarge                          us-east-2a                       No              300 GiB
----
endif::[]
ifeval::["{rosa_deploy_hcp}" == "true"]
.Sample Output
[source,text,options=nowrap]
----
ID       AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONE  SUBNET                    VERSION  AUTOREPAIR  
workers  No           2/2       m5.xlarge                          us-east-2a         subnet-02ee20ca64bb93535  4.14.1   Yes
----
endif::[]

ifeval::["{rosa_deploy_hcp}" == "false"]
. Now, let's take a look at the MachineSets inside of the ROSA cluster that have been created according to the instructions provided by the above MachinePools. To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machinesets
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
rosa-6n4s8-7hbhw-infra-us-east-2a      2         2         2       2           22h
rosa-6n4s8-7hbhw-worker-us-east-2a     2         2         2       2           23h
----
+
You will see two MachineSets, one for worker nodes and one for infra nodes.

. Now, let's take a look at the Machines inside of the ROSA cluster that have been created according to the instructions provided by the above MachineSets.
To do so, run the following command:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machine
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                       PHASE     TYPE         REGION      ZONE         AGE
rosa-6n4s8-7hbhw-infra-us-east-2a-bgr86    Running   r5.xlarge    us-east-2   us-east-2a   22h
rosa-6n4s8-7hbhw-infra-us-east-2a-xthr6    Running   r5.xlarge    us-east-2   us-east-2a   22h
rosa-6n4s8-7hbhw-master-0                  Running   m5.2xlarge   us-east-2   us-east-2a   23h
rosa-6n4s8-7hbhw-master-1                  Running   m5.2xlarge   us-east-2   us-east-2a   23h
rosa-6n4s8-7hbhw-master-2                  Running   m5.2xlarge   us-east-2   us-east-2a   23h
rosa-6n4s8-7hbhw-worker-us-east-2a-xc8g2   Running   m5.xlarge    us-east-2   us-east-2a   22h
rosa-6n4s8-7hbhw-worker-us-east-2a-zxm8j   Running   m5.xlarge    us-east-2   us-east-2a   22h
----
+
For this workshop, we've deployed your ROSA cluster with seven total machines (two workers, three control planes, and two infrastructure nodes).

. Now that we know that we have two worker nodes, let's create a MachinePool to add a new worker node using the ROSA CLI. For the additional machine pool we are using AWS Spot Instances - those are much cheaper than regular instances - but may disappear at a few minutes notice. Which isn't really a big problem for our use case because the machine pool would immediately create a new one when one disappears.
+
To create the machine pool, run the following command:
+
[source,sh,role=execute]
----
rosa create machinepool -c rosa-$GUID --replicas 1 --name workshop --instance-type m5.xlarge --use-spot-instances
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Fetching instance types
I: Machine pool 'workshop' created successfully on cluster 'rosa-82prr'
I: To view all machine pools, run 'rosa list machinepools -c rosa-82prr'
----
+
This command adds a single m5.xlarge instance to the first AWS availability zone in the region your cluster is deployed in.
endif::[]

. Now, let's scale up our MachinePool from two to three machines.
To do so, run the following command:
ifeval::["{rosa_deploy_hcp}" == "false"]
+
[source,sh,role=execute]
----
rosa update machinepool -c rosa-$GUID --replicas 3 workshop
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Updated machine pool 'workshop' on cluster 'rosa-6n4s8'
----
endif::[]
ifeval::["{rosa_deploy_hcp}" == "true"]
+
[source,sh,role=execute]
----
rosa update machinepool -c rosa-$GUID --replicas 3 workers
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Updated machine pool 'workers' on hosted cluster 'rosa-zwrzl'
----

. It will take about 5 minutes for the additional worker node to be available. You can either continue to the next step - or if you want to see the worker node just run the following command until you see three worker nodes (then hit `Ctrl-C` to abort the watch):
+
[source,sh,role=execute]
----
watch -n 10 oc get nodes
----
+
.Sample Output
[source,text,options=nowrap]
----
Every 10.0s: oc get nodes                               bastion.7v44k.internal: Fri Nov  3 07:47:52 2023

NAME                                      STATUS   ROLES    AGE    VERSION
ip-10-0-0-29.us-east-2.compute.internal   Ready    worker   115m   v1.27.6+f67aeb3
ip-10-0-0-59.us-east-2.compute.internal   Ready    worker   71s    v1.27.6+f67aeb3
ip-10-0-0-62.us-east-2.compute.internal   Ready    worker   115m   v1.27.6+f67aeb3
----
endif::[]

ifeval::["{rosa_deploy_hcp}" == "false"]
. Now that we've scaled the MachinePool to three machines, we can see that the machines are already being created.
First, let's quickly check the output of the `oc get machinesets` command we ran earlier:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machinesets
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                    DESIRED   CURRENT   READY   AVAILABLE   AGE
rosa-82prr-dw6cz-infra-us-east-2a       2         2         2       2           3h12m
rosa-82prr-dw6cz-worker-us-east-2a      2         2         2       2           3h32m
rosa-82prr-dw6cz-workshop-us-east-2a    3         3         1       1           21m
----
+
Note, that the number of *desired* and *current* nodes matches the scale we specified, but depending when you ran this command they may not be available yet.

. We can also get the state of our machines to see the additional machines being provisioned:
+
[source,sh,role=execute]
----
watch -n 10 oc -n openshift-machine-api get machine
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                          PHASE         TYPE         REGION      ZONE         AGE
rosa-82prr-dw6cz-infra-us-east-2a-cbklb       Running       r5.xlarge    us-east-2   us-east-2a   78m
rosa-82prr-dw6cz-infra-us-east-2a-f6v8n       Running       r5.xlarge    us-east-2   us-east-2a   3h13m
rosa-82prr-dw6cz-master-0                     Running       m5.2xlarge   us-east-2   us-east-2a   3h33m
rosa-82prr-dw6cz-master-1                     Running       m5.2xlarge   us-east-2   us-east-2a   3h33m
rosa-82prr-dw6cz-master-2                     Running       m5.2xlarge   us-east-2   us-east-2a   3h33m
rosa-82prr-dw6cz-worker-us-east-2a-g8f5m      Running       m5.xlarge    us-east-2   us-east-2a   3h29m
rosa-82prr-dw6cz-worker-us-east-2a-stwdg      Running       m5.xlarge    us-east-2   us-east-2a   3h29m
rosa-82prr-dw6cz-workshop-us-east-2a-2wqsr    Provisioned   m5.xlarge    us-east-2   us-east-2a   3m18s
rosa-82prr-dw6cz-workshop-us-east-2a-s6hpf    Running       m5.xlarge    us-east-2   us-east-2a   22m
rosa-82prr-dw6cz-workshop-us-east-2a-sstzd    Provisioned   m5.xlarge    us-east-2   us-east-2a   3m18s
----

. Let the above command run until all machines are in the *Running* phase. This means that they are ready and available to run Pods in the cluster. Hit `CTRL-C` to exit the `oc` command.
endif::[]

. We don't actually need this extra worker node so let's scale the cluster back down to a total of 2 worker nodes by scaling down the Machine Pool.
+
To do so, run the following command:
+
ifeval::["{rosa_deploy_hcp}" == "false"]
[source,sh,role=execute]
----
rosa update machinepool -c rosa-$GUID --replicas 2 workshop
----
endif::[]
ifeval::["{rosa_deploy_hcp}" == "true"]
[source,sh,role=execute]
----
rosa update machinepool -c rosa-$GUID --replicas 2 workers
----
+
If you want to wait until the additional node has been removed repeat the previous command (`oc get nodes`) until you see just two worker nodes again.
endif::[]

ifeval::["{rosa_deploy_hcp}" == "false"]
. Now that we've scaled the MachinePool (and therefore the MachineSet) back down to one machine, we can see the change reflected in the cluster almost immediately.
Let's quickly check the output of the same command we ran before:
+
[source,sh,role=execute]
----
oc -n openshift-machine-api get machinesets
----
+
.Sample Output
[source,text,options=nowrap]
----
NAME                                    DESIRED   CURRENT   READY   AVAILABLE   AGE
rosa-82prr-dw6cz-infra-us-east-2a       2         2         2       2           3h15m
rosa-82prr-dw6cz-worker-us-east-2a      2         2         2       2           3h35m
rosa-82prr-dw6cz-workshop-us-east-2a    2         2         2       2           25m
----

. Now let's scale the cluster back down to a total of 2 worker nodes by deleting the "workshop" Machine Pool.
+
To do so, run the following command:
+
[source,sh,role=execute]
----
rosa delete machinepool -c rosa-$GUID workshop --yes
----
+
.Sample Output
[source,text,options=nowrap]
----
I: Successfully deleted machine pool 'workshop' from cluster 'rosa-6n4s8'
----
+
. You can validate that the MachinePool has been deleted by using the `rosa` cli:
+
[source,sh,role=execute]
----
rosa list machinepools -c rosa-$GUID
----
+
.Sample Output
[source,text,options=nowrap]
----
ID      AUTOSCALING  REPLICAS  INSTANCE TYPE  LABELS    TAINTS    AVAILABILITY ZONES    SUBNETS    SPOT INSTANCES  DISK SIZE  
worker  No           2         m5.xlarge                          us-east-2a                       No              300 GiB
----
endif::[]

*Congratulations!*

You've successfully scaled your cluster up and back down to two worker nodes.

== Summary

Here you learned:

ifeval::["{rosa_deploy_hcp}" == "false"]
* Creating new Machine Pool for your ROSA cluster to add additional nodes to the cluster
endif::[]
* Scaling a Machine Pool up to add more nodes to the cluster
* Scaling a Machine Pool down to remove worker nodes from the cluster
